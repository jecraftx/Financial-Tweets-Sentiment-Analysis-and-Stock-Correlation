{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ddadb401-102f-4f2a-b3a8-5ce3ab5b2796",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q torch torchvision transformers tqdm scikit-learn matplotlib ipywidgets timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11007569-7dfc-42f9-ba00-812af52dfeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.transforms import Compose, RandomHorizontalFlip, RandomRotation, Resize, CenterCrop, ToTensor, Normalize, RandomResizedCrop\n",
    "from torchvision.models import efficientnet_v2_s, EfficientNet_V2_S_Weights\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from torch.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81e47a93-c838-4f31-a3fe-19c77f1b77e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://data.vision.ee.ethz.ch/cvl/food-101.tar.gz to ./data/food-101.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.00G/5.00G [08:01<00:00, 10.4MB/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/food-101.tar.gz to ./data\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "data_root = './data'\n",
    "\n",
    "full_train_dataset = datasets.Food101(\n",
    "    root=data_root,\n",
    "    split='train',\n",
    "    download=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37ce0e1b-95f4-4561-abc7-ec8e1caec0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(full_train_dataset))  # Training: 80%\n",
    "val_size = len(full_train_dataset) - train_size  # Validation: 20%\n",
    "\n",
    "# Dataset Split\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "# Load Test Dataset\n",
    "test_dataset = datasets.Food101(\n",
    "    root=data_root,\n",
    "    split='test',\n",
    "    download=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9afe708c-cded-4656-8281-4b7aa7663c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e38e6f91df8d4ff9a9eb031b6f08bfeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "class ViTTransform:\n",
    "    def __init__(self, feature_processor, augment=False):\n",
    "        self.feature_processor = feature_processor\n",
    "        if augment:\n",
    "            self.augment = Compose([\n",
    "                RandomHorizontalFlip(),\n",
    "                RandomRotation(10),\n",
    "            ])\n",
    "        else:\n",
    "            self.augment = None\n",
    "\n",
    "    def __call__(self, image):\n",
    "        if self.augment:\n",
    "            image = self.augment(image)\n",
    "        encoding = self.feature_processor(images=image, return_tensors=\"pt\")\n",
    "        return encoding['pixel_values'].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a7bce0a-4924-4716-a604-c3014a43ddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_dataset.transform = ViTTransform(feature_processor, augment=True)\n",
    "test_dataset.transform = ViTTransform(feature_processor, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd8b48b7-c7fc-4d39-9be3-4c1ab77f9969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure DataLoader\n",
    "batch_size = 64\n",
    "num_workers = os.cpu_count()\n",
    "epochs = 10  \n",
    "patience = 3 \n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "validation_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2ceb03d-fae0-4209-8fb2-3a793be33453",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa2a8399f3db436db984d2e7c9c8cc7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51c6554531446919e64fd097ace3079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([101]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([101, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTSdpaAttention(\n",
       "            (attention): ViTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=101, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Set CUDA Optimization\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Initialize ViT Model\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    'google/vit-base-patch16-224',\n",
    "    num_labels=101,  # Food101 클래스 수로 설정\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# Set ID and Label Mapping (from Original Dataset)\n",
    "model.config.id2label = {str(i): label for i, label in enumerate(full_train_dataset.classes)}\n",
    "model.config.label2id = {label: i for i, label in enumerate(full_train_dataset.classes)}\n",
    "\n",
    "# Freeze Parameters: Fix all parameters of the model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Set only the last classifier layer to be trainable\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Move the model to the device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cc739aa-f879-481e-8fd5-38962d660b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_training_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,  # Number of warm-up steps\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "scaler = GradScaler('cuda')\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "early_stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "410beb5e-b689-4b56-b0b5-953d82472d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Training] - Loss: 2.8362 | Acc: 0.5006 | F1: 0.5000 | Precision: 0.5269 | Recall: 0.5006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Validation] - Loss: 1.7542 | Acc: 0.6931 | F1: 0.6865 | Precision: 0.6947 | Recall: 0.6931\n",
      "Validation loss improved. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Training] - Loss: 1.4293 | Acc: 0.7245 | F1: 0.7216 | Precision: 0.7240 | Recall: 0.7245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Validation] - Loss: 1.2461 | Acc: 0.7363 | F1: 0.7338 | Precision: 0.7376 | Recall: 0.7363\n",
      "Validation loss improved. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Training] - Loss: 1.1284 | Acc: 0.7527 | F1: 0.7513 | Precision: 0.7529 | Recall: 0.7527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Validation] - Loss: 1.0830 | Acc: 0.7513 | F1: 0.7500 | Precision: 0.7527 | Recall: 0.7513\n",
      "Validation loss improved. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Training] - Loss: 1.0058 | Acc: 0.7661 | F1: 0.7653 | Precision: 0.7665 | Recall: 0.7661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Validation] - Loss: 1.0004 | Acc: 0.7628 | F1: 0.7615 | Precision: 0.7638 | Recall: 0.7628\n",
      "Validation loss improved. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Training] - Loss: 0.9421 | Acc: 0.7719 | F1: 0.7712 | Precision: 0.7721 | Recall: 0.7719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Validation] - Loss: 0.9567 | Acc: 0.7686 | F1: 0.7681 | Precision: 0.7702 | Recall: 0.7686\n",
      "Validation loss improved. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Training] - Loss: 0.9019 | Acc: 0.7794 | F1: 0.7788 | Precision: 0.7795 | Recall: 0.7794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Validation] - Loss: 0.9238 | Acc: 0.7716 | F1: 0.7707 | Precision: 0.7724 | Recall: 0.7716\n",
      "Validation loss improved. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Training] - Loss: 0.8729 | Acc: 0.7818 | F1: 0.7812 | Precision: 0.7821 | Recall: 0.7818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Validation] - Loss: 0.9040 | Acc: 0.7758 | F1: 0.7752 | Precision: 0.7771 | Recall: 0.7758\n",
      "Validation loss improved. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Training] - Loss: 0.8569 | Acc: 0.7852 | F1: 0.7847 | Precision: 0.7854 | Recall: 0.7852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Validation] - Loss: 0.8976 | Acc: 0.7758 | F1: 0.7752 | Precision: 0.7771 | Recall: 0.7758\n",
      "Validation loss improved. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Training] - Loss: 0.8445 | Acc: 0.7880 | F1: 0.7875 | Precision: 0.7881 | Recall: 0.7880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Validation] - Loss: 0.8913 | Acc: 0.7782 | F1: 0.7771 | Precision: 0.7788 | Recall: 0.7782\n",
      "Validation loss improved. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Training] - Loss: 0.8392 | Acc: 0.7891 | F1: 0.7885 | Precision: 0.7891 | Recall: 0.7891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_366/4221171825.py:102: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('ViT_best_model_state.bin'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Validation] - Loss: 0.8929 | Acc: 0.7745 | F1: 0.7735 | Precision: 0.7751 | Recall: 0.7745\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Training complete and model saved.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    if early_stop:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "    # Training steps\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    loop = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} [Training]', leave=False)\n",
    "    for batch in loop:\n",
    "        inputs, labels = batch  # Batch Unpacking\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast('cuda'):  # Use modified autocast\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Update statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs.logits, 1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "        # Save predictions and actual labels\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_acc = correct_predictions.double() / total_predictions\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    print(f'Epoch {epoch+1}/{epochs} [Training] - Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.4f} | F1: {f1:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}')\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loop = tqdm(validation_loader, desc=f'Epoch {epoch+1}/{epochs} [Validation]', leave=False)\n",
    "        for batch in loop:\n",
    "            inputs, labels = batch  # Batch Unpacking\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            with autocast('cuda'):  # Use modified autocast\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.logits, labels)\n",
    "\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs.logits, 1)\n",
    "            val_correct += torch.sum(preds == labels)\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "            # Save predictions and actual labels\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_epoch_loss = val_loss / len(validation_loader.dataset)\n",
    "    val_epoch_acc = val_correct.double() / val_total\n",
    "    val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
    "    val_precision = precision_score(val_labels, val_preds, average='weighted')\n",
    "    val_recall = recall_score(val_labels, val_preds, average='weighted')\n",
    "    print(f'Epoch {epoch+1}/{epochs} [Validation] - Loss: {val_epoch_loss:.4f} | Acc: {val_epoch_acc:.4f} | F1: {val_f1:.4f} | Precision: {val_precision:.4f} | Recall: {val_recall:.4f}')\n",
    "\n",
    "    # Early Stopping check\n",
    "    if val_epoch_loss < best_val_loss:\n",
    "        best_val_loss = val_epoch_loss\n",
    "        epochs_no_improve = 0\n",
    "        # Save model\n",
    "        torch.save(model.state_dict(), 'ViT_best_model_state.bin')\n",
    "        print(f'Validation loss improved. Saving model...')\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f'No improvement in validation loss for {epochs_no_improve} epoch(s).')\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f'Early stopping triggered after {patience} epochs with no improvement.')\n",
    "            early_stop = True\n",
    "\n",
    "# Load model after training completion\n",
    "model.load_state_dict(torch.load('ViT_best_model_state.bin'))\n",
    "\n",
    "# Save the model and feature processor (changed to ViTImageProcessor)\n",
    "model.save_pretrained('./vit-food101')\n",
    "feature_processor.save_pretrained('./vit-food101')\n",
    "\n",
    "print(\"Training complete and model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9a26049-5f49-4d80-947d-183e7920f0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test Evaluation] - Loss: 0.6902 | Acc: 0.8282 | F1: 0.8279 | Precision: 0.8293 | Recall: 0.8282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Load the model and Feature Extractor using from_pretrained\n",
    "model = ViTForImageClassification.from_pretrained('./vit-food101')\n",
    "feature_processor = ViTImageProcessor.from_pretrained('./vit-food101')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move to device\n",
    "model.to(device)\n",
    "\n",
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    loop = tqdm(test_loader, desc='[Test Evaluation]', leave=False)\n",
    "    for batch in loop:\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs.logits, 1)\n",
    "        test_correct += torch.sum(preds == labels)\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "        # Save predictions and actual labels\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_epoch_loss = test_loss / len(test_dataset)\n",
    "test_epoch_acc = test_correct.double() / test_total\n",
    "test_f1 = f1_score(test_labels, test_preds, average='weighted')\n",
    "test_precision = precision_score(test_labels, test_preds, average='weighted')\n",
    "test_recall = recall_score(test_labels, test_preds, average='weighted')\n",
    "print(f'[Test Evaluation] - Loss: {test_epoch_loss:.4f} | Acc: {test_epoch_acc:.4f} | F1: {test_f1:.4f} | Precision: {test_precision:.4f} | Recall: {test_recall:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db31b0e6-001d-44f9-a8b7-c0b07495509f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "\n",
    "from torchvision.models import efficientnet_v2_s, EfficientNet_V2_S_Weights\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e655700-96e1-42ab-85fd-09d8ae7f3b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 101\n"
     ]
    }
   ],
   "source": [
    "# Data root directory\n",
    "data_root = './data'\n",
    "\n",
    "# Define transforms\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(384),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize(384),\n",
    "    transforms.CenterCrop(384),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load the datasets with ImageFolder\n",
    "full_train_dataset = datasets.Food101(\n",
    "    root=data_root,\n",
    "    split='train',\n",
    "    transform=train_transforms,\n",
    "    download=False\n",
    ")\n",
    "\n",
    "# Split into training and validation\n",
    "train_size = int(0.8 * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "# Apply validation transforms to the validation dataset\n",
    "val_dataset.dataset.transform = val_test_transforms\n",
    "\n",
    "# Load test dataset\n",
    "test_dataset = datasets.Food101(\n",
    "    root=data_root,\n",
    "    split='test',\n",
    "    transform=val_test_transforms,\n",
    "    download=False\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Class names\n",
    "class_names = full_train_dataset.classes\n",
    "num_classes = len(class_names)\n",
    "print(f'Number of classes: {num_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b5a0a159-c4b3-43a8-b1f8-b84ed65b7e71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: ['features.6.0.block.0.0.weight', 'features.6.0.block.0.1.weight', 'features.6.0.block.0.1.bias', 'features.6.0.block.1.0.weight', 'features.6.0.block.1.1.weight', 'features.6.0.block.1.1.bias', 'features.6.0.block.2.fc1.weight', 'features.6.0.block.2.fc1.bias', 'features.6.0.block.2.fc2.weight', 'features.6.0.block.2.fc2.bias', 'features.6.0.block.3.0.weight', 'features.6.0.block.3.1.weight', 'features.6.0.block.3.1.bias', 'features.6.1.block.0.0.weight', 'features.6.1.block.0.1.weight', 'features.6.1.block.0.1.bias', 'features.6.1.block.1.0.weight', 'features.6.1.block.1.1.weight', 'features.6.1.block.1.1.bias', 'features.6.1.block.2.fc1.weight', 'features.6.1.block.2.fc1.bias', 'features.6.1.block.2.fc2.weight', 'features.6.1.block.2.fc2.bias', 'features.6.1.block.3.0.weight', 'features.6.1.block.3.1.weight', 'features.6.1.block.3.1.bias', 'features.6.2.block.0.0.weight', 'features.6.2.block.0.1.weight', 'features.6.2.block.0.1.bias', 'features.6.2.block.1.0.weight', 'features.6.2.block.1.1.weight', 'features.6.2.block.1.1.bias', 'features.6.2.block.2.fc1.weight', 'features.6.2.block.2.fc1.bias', 'features.6.2.block.2.fc2.weight', 'features.6.2.block.2.fc2.bias', 'features.6.2.block.3.0.weight', 'features.6.2.block.3.1.weight', 'features.6.2.block.3.1.bias', 'features.6.3.block.0.0.weight', 'features.6.3.block.0.1.weight', 'features.6.3.block.0.1.bias', 'features.6.3.block.1.0.weight', 'features.6.3.block.1.1.weight', 'features.6.3.block.1.1.bias', 'features.6.3.block.2.fc1.weight', 'features.6.3.block.2.fc1.bias', 'features.6.3.block.2.fc2.weight', 'features.6.3.block.2.fc2.bias', 'features.6.3.block.3.0.weight', 'features.6.3.block.3.1.weight', 'features.6.3.block.3.1.bias', 'features.6.4.block.0.0.weight', 'features.6.4.block.0.1.weight', 'features.6.4.block.0.1.bias', 'features.6.4.block.1.0.weight', 'features.6.4.block.1.1.weight', 'features.6.4.block.1.1.bias', 'features.6.4.block.2.fc1.weight', 'features.6.4.block.2.fc1.bias', 'features.6.4.block.2.fc2.weight', 'features.6.4.block.2.fc2.bias', 'features.6.4.block.3.0.weight', 'features.6.4.block.3.1.weight', 'features.6.4.block.3.1.bias', 'features.6.5.block.0.0.weight', 'features.6.5.block.0.1.weight', 'features.6.5.block.0.1.bias', 'features.6.5.block.1.0.weight', 'features.6.5.block.1.1.weight', 'features.6.5.block.1.1.bias', 'features.6.5.block.2.fc1.weight', 'features.6.5.block.2.fc1.bias', 'features.6.5.block.2.fc2.weight', 'features.6.5.block.2.fc2.bias', 'features.6.5.block.3.0.weight', 'features.6.5.block.3.1.weight', 'features.6.5.block.3.1.bias', 'features.6.6.block.0.0.weight', 'features.6.6.block.0.1.weight', 'features.6.6.block.0.1.bias', 'features.6.6.block.1.0.weight', 'features.6.6.block.1.1.weight', 'features.6.6.block.1.1.bias', 'features.6.6.block.2.fc1.weight', 'features.6.6.block.2.fc1.bias', 'features.6.6.block.2.fc2.weight', 'features.6.6.block.2.fc2.bias', 'features.6.6.block.3.0.weight', 'features.6.6.block.3.1.weight', 'features.6.6.block.3.1.bias', 'features.6.7.block.0.0.weight', 'features.6.7.block.0.1.weight', 'features.6.7.block.0.1.bias', 'features.6.7.block.1.0.weight', 'features.6.7.block.1.1.weight', 'features.6.7.block.1.1.bias', 'features.6.7.block.2.fc1.weight', 'features.6.7.block.2.fc1.bias', 'features.6.7.block.2.fc2.weight', 'features.6.7.block.2.fc2.bias', 'features.6.7.block.3.0.weight', 'features.6.7.block.3.1.weight', 'features.6.7.block.3.1.bias', 'features.6.8.block.0.0.weight', 'features.6.8.block.0.1.weight', 'features.6.8.block.0.1.bias', 'features.6.8.block.1.0.weight', 'features.6.8.block.1.1.weight', 'features.6.8.block.1.1.bias', 'features.6.8.block.2.fc1.weight', 'features.6.8.block.2.fc1.bias', 'features.6.8.block.2.fc2.weight', 'features.6.8.block.2.fc2.bias', 'features.6.8.block.3.0.weight', 'features.6.8.block.3.1.weight', 'features.6.8.block.3.1.bias', 'features.6.9.block.0.0.weight', 'features.6.9.block.0.1.weight', 'features.6.9.block.0.1.bias', 'features.6.9.block.1.0.weight', 'features.6.9.block.1.1.weight', 'features.6.9.block.1.1.bias', 'features.6.9.block.2.fc1.weight', 'features.6.9.block.2.fc1.bias', 'features.6.9.block.2.fc2.weight', 'features.6.9.block.2.fc2.bias', 'features.6.9.block.3.0.weight', 'features.6.9.block.3.1.weight', 'features.6.9.block.3.1.bias', 'features.6.10.block.0.0.weight', 'features.6.10.block.0.1.weight', 'features.6.10.block.0.1.bias', 'features.6.10.block.1.0.weight', 'features.6.10.block.1.1.weight', 'features.6.10.block.1.1.bias', 'features.6.10.block.2.fc1.weight', 'features.6.10.block.2.fc1.bias', 'features.6.10.block.2.fc2.weight', 'features.6.10.block.2.fc2.bias', 'features.6.10.block.3.0.weight', 'features.6.10.block.3.1.weight', 'features.6.10.block.3.1.bias', 'features.6.11.block.0.0.weight', 'features.6.11.block.0.1.weight', 'features.6.11.block.0.1.bias', 'features.6.11.block.1.0.weight', 'features.6.11.block.1.1.weight', 'features.6.11.block.1.1.bias', 'features.6.11.block.2.fc1.weight', 'features.6.11.block.2.fc1.bias', 'features.6.11.block.2.fc2.weight', 'features.6.11.block.2.fc2.bias', 'features.6.11.block.3.0.weight', 'features.6.11.block.3.1.weight', 'features.6.11.block.3.1.bias', 'features.6.12.block.0.0.weight', 'features.6.12.block.0.1.weight', 'features.6.12.block.0.1.bias', 'features.6.12.block.1.0.weight', 'features.6.12.block.1.1.weight', 'features.6.12.block.1.1.bias', 'features.6.12.block.2.fc1.weight', 'features.6.12.block.2.fc1.bias', 'features.6.12.block.2.fc2.weight', 'features.6.12.block.2.fc2.bias', 'features.6.12.block.3.0.weight', 'features.6.12.block.3.1.weight', 'features.6.12.block.3.1.bias', 'features.6.13.block.0.0.weight', 'features.6.13.block.0.1.weight', 'features.6.13.block.0.1.bias', 'features.6.13.block.1.0.weight', 'features.6.13.block.1.1.weight', 'features.6.13.block.1.1.bias', 'features.6.13.block.2.fc1.weight', 'features.6.13.block.2.fc1.bias', 'features.6.13.block.2.fc2.weight', 'features.6.13.block.2.fc2.bias', 'features.6.13.block.3.0.weight', 'features.6.13.block.3.1.weight', 'features.6.13.block.3.1.bias', 'features.6.14.block.0.0.weight', 'features.6.14.block.0.1.weight', 'features.6.14.block.0.1.bias', 'features.6.14.block.1.0.weight', 'features.6.14.block.1.1.weight', 'features.6.14.block.1.1.bias', 'features.6.14.block.2.fc1.weight', 'features.6.14.block.2.fc1.bias', 'features.6.14.block.2.fc2.weight', 'features.6.14.block.2.fc2.bias', 'features.6.14.block.3.0.weight', 'features.6.14.block.3.1.weight', 'features.6.14.block.3.1.bias', 'features.7.0.weight', 'features.7.1.weight', 'features.7.1.bias', 'classifier.1.weight', 'classifier.1.bias']\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained EfficientNet_V2_S model\n",
    "model = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.DEFAULT)\n",
    "\n",
    "# Modify the classifier to match the number of classes in Food101\n",
    "# The classifier is typically the last layer\n",
    "num_ftrs = model.classifier[1].in_features\n",
    "model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "# Move the model to the device\n",
    "model = model.to(device)\n",
    "\n",
    "# Freeze all layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Retrieve the layers from the model\n",
    "layers = list(model.features.children())\n",
    "\n",
    "for layer in layers[-2:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "trainable_params = [name for name, param in model.named_parameters() if param.requires_grad]\n",
    "print(f'Trainable parameters: {trainable_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "12f362ae-2396-43d6-858b-76b9b5b6d642",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Collect only the parameters that require gradients\n",
    "trainable_parameters = [param for param in model.parameters() if param.requires_grad]\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(trainable_parameters, lr=5e-5)\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "37f5a956-7a03-4828-aa9e-9da06f5af762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    scheduler, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    device, \n",
    "    num_epochs=10, \n",
    "    patience=3\n",
    "):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "\n",
    "    scaler = GradScaler()  # Initialize scaler for mixed precision training\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if early_stop:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "        print(f'Epoch {epoch +1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Training step\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        loop = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Training]', leave=False)\n",
    "        for batch in loop:\n",
    "            inputs, labels = batch  # Batch unpacking\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast('cuda'):  # Use mixed precision\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)  # EfficientNet returns outputs, not outputs.logits\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # Update statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "            # Save predictions and actual labels\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = correct_predictions.double() / total_predictions\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "        recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "        print(f'[Training] Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.4f} | F1: {f1:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}')\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loop = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Validation]', leave=False)\n",
    "            for batch in loop:\n",
    "                inputs, labels = batch  # Batch unpacking\n",
    "                inputs = inputs.to(device, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "                with autocast('cuda'):  # Use mixed precision\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)  # EfficientNet returns outputs, not outputs.logits\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_correct += torch.sum(preds == labels)\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "                # Save predictions and actual labels\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_epoch_loss = val_loss / len(val_loader.dataset)\n",
    "        val_epoch_acc = val_correct.double() / val_total\n",
    "        val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
    "        val_precision = precision_score(val_labels, val_preds, average='weighted')\n",
    "        val_recall = recall_score(val_labels, val_preds, average='weighted')\n",
    "        print(f'[Validation] Loss: {val_epoch_loss:.4f} | Acc: {val_epoch_acc:.4f} | F1: {val_f1:.4f} | Precision: {val_precision:.4f} | Recall: {val_recall:.4f}')\n",
    "\n",
    "        # Scheduler step (typically after validation epoch)\n",
    "        scheduler.step()\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_epoch_loss < best_val_loss:\n",
    "            best_val_loss = val_epoch_loss\n",
    "            epochs_no_improve = 0\n",
    "            # Save model\n",
    "            torch.save(model.state_dict(), 'best_model_state.pth')\n",
    "            print(f'Validation loss improved. Saving model...')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f'Validation loss has not improved for {epochs_no_improve} epoch(s).')\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f'Early stopping triggered after {patience} epochs with no improvement.')\n",
    "                early_stop = True\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training completed in: {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best validation accuracy: {best_acc:.4f}')\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(torch.load('best_model_state.pth', weights_only=True))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f208b27b-28db-42bc-ab4b-eeedefe40459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training] Loss: 2.3790 | Acc: 0.4872 | F1: 0.4800 | Precision: 0.4911 | Recall: 0.4872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: 1.1013 | Acc: 0.7185 | F1: 0.7144 | Precision: 0.7178 | Recall: 0.7185\n",
      "검증 손실이 개선되었습니다. 모델을 저장합니다...\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training] Loss: 0.9743 | Acc: 0.7519 | F1: 0.7508 | Precision: 0.7506 | Recall: 0.7519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: 0.8306 | Acc: 0.7804 | F1: 0.7792 | Precision: 0.7836 | Recall: 0.7804\n",
      "검증 손실이 개선되었습니다. 모델을 저장합니다...\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training] Loss: 0.6752 | Acc: 0.8222 | F1: 0.8218 | Precision: 0.8219 | Recall: 0.8222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: 0.7568 | Acc: 0.7993 | F1: 0.7983 | Precision: 0.8003 | Recall: 0.7993\n",
      "검증 손실이 개선되었습니다. 모델을 저장합니다...\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training] Loss: 0.4886 | Acc: 0.8705 | F1: 0.8704 | Precision: 0.8706 | Recall: 0.8705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: 0.7452 | Acc: 0.8059 | F1: 0.8050 | Precision: 0.8072 | Recall: 0.8059\n",
      "검증 손실이 개선되었습니다. 모델을 저장합니다...\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training] Loss: 0.3619 | Acc: 0.9033 | F1: 0.9033 | Precision: 0.9034 | Recall: 0.9033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: 0.7420 | Acc: 0.8139 | F1: 0.8134 | Precision: 0.8154 | Recall: 0.8139\n",
      "검증 손실이 개선되었습니다. 모델을 저장합니다...\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training] Loss: 0.2747 | Acc: 0.9265 | F1: 0.9264 | Precision: 0.9265 | Recall: 0.9265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: 0.7652 | Acc: 0.8125 | F1: 0.8112 | Precision: 0.8140 | Recall: 0.8125\n",
      "검증 손실이 1 에폭 동안 개선되지 않았습니다.\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training] Loss: 0.2060 | Acc: 0.9440 | F1: 0.9440 | Precision: 0.9441 | Recall: 0.9440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: 0.7823 | Acc: 0.8150 | F1: 0.8148 | Precision: 0.8173 | Recall: 0.8150\n",
      "검증 손실이 2 에폭 동안 개선되지 않았습니다.\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training] Loss: 0.1462 | Acc: 0.9618 | F1: 0.9617 | Precision: 0.9618 | Recall: 0.9618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: 0.7710 | Acc: 0.8224 | F1: 0.8225 | Precision: 0.8239 | Recall: 0.8224\n",
      "검증 손실이 3 에폭 동안 개선되지 않았습니다.\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training] Loss: 0.1236 | Acc: 0.9684 | F1: 0.9684 | Precision: 0.9684 | Recall: 0.9684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: 0.7886 | Acc: 0.8236 | F1: 0.8232 | Precision: 0.8243 | Recall: 0.8236\n",
      "검증 손실이 4 에폭 동안 개선되지 않았습니다.\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training] Loss: 0.1181 | Acc: 0.9704 | F1: 0.9704 | Precision: 0.9705 | Recall: 0.9704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Loss: 0.7935 | Acc: 0.8236 | F1: 0.8234 | Precision: 0.8249 | Recall: 0.8236\n",
      "검증 손실이 5 에폭 동안 개선되지 않았습니다.\n",
      "5 에폭 동안 개선이 없어서 조기 종료를 트리거합니다.\n",
      "\n",
      "학습 완료까지 걸린 시간: 38분 38초\n",
      "최고 검증 정확도: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Device setting\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Run training\n",
    "model = train_model(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    num_epochs=10,\n",
    "    patience=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6a41ccb2-175e-47bf-9adc-f7c727955a51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EfficientNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU(inplace=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): FusedMBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
      "      )\n",
      "      (1): FusedMBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.005, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): FusedMBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(24, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.01, mode=row)\n",
      "      )\n",
      "      (1): FusedMBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.015000000000000003, mode=row)\n",
      "      )\n",
      "      (2): FusedMBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.02, mode=row)\n",
      "      )\n",
      "      (3): FusedMBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): FusedMBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.030000000000000006, mode=row)\n",
      "      )\n",
      "      (1): FusedMBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.035, mode=row)\n",
      "      )\n",
      "      (2): FusedMBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.04, mode=row)\n",
      "      )\n",
      "      (3): FusedMBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.045, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.05500000000000001, mode=row)\n",
      "      )\n",
      "      (2): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.06000000000000001, mode=row)\n",
      "      )\n",
      "      (3): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.065, mode=row)\n",
      "      )\n",
      "      (4): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.07, mode=row)\n",
      "      )\n",
      "      (5): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.075, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
      "            (1): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.08, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.085, mode=row)\n",
      "      )\n",
      "      (2): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.09, mode=row)\n",
      "      )\n",
      "      (3): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.095, mode=row)\n",
      "      )\n",
      "      (4): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
      "      )\n",
      "      (5): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.10500000000000001, mode=row)\n",
      "      )\n",
      "      (6): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.11000000000000001, mode=row)\n",
      "      )\n",
      "      (7): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.11500000000000002, mode=row)\n",
      "      )\n",
      "      (8): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.12000000000000002, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=960, bias=False)\n",
      "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
      "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.13, mode=row)\n",
      "      )\n",
      "      (2): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
      "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.135, mode=row)\n",
      "      )\n",
      "      (3): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
      "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.14, mode=row)\n",
      "      )\n",
      "      (4): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
      "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.14500000000000002, mode=row)\n",
      "      )\n",
      "      (5): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
      "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.15, mode=row)\n",
      "      )\n",
      "      (6): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
      "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.155, mode=row)\n",
      "      )\n",
      "      (7): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
      "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.16, mode=row)\n",
      "      )\n",
      "      (8): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
      "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.165, mode=row)\n",
      "      )\n",
      "      (9): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
      "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.17, mode=row)\n",
      "      )\n",
      "      (10): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
      "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.175, mode=row)\n",
      "      )\n",
      "      (11): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
      "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.18, mode=row)\n",
      "      )\n",
      "      (12): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
      "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.185, mode=row)\n",
      "      )\n",
      "      (13): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
      "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.19, mode=row)\n",
      "      )\n",
      "      (14): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
      "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.195, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (7): Conv2dNormActivation(\n",
      "      (0): Conv2d(256, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1280, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=True)\n",
      "    (1): Linear(in_features=1280, out_features=101, bias=True)\n",
      "  )\n",
      ")\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "# Save model\n",
    "torch.save(model.state_dict(), './efficientnet_food101.pth')\n",
    "\n",
    "# Load model\n",
    "model.load_state_dict(torch.load('./efficientnet_food101.pth', weights_only=True))\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6b5dc463-d83e-496c-b5e8-eb4179b4dfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating EfficientNetV2 Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EfficientNetV2 Test Evaluation] - Loss: 0.5197 | Acc: 0.8613 | F1: 0.8610 | Precision: 0.8621 | Recall: 0.8613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Device setting\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model.load_state_dict(torch.load('./efficientnet_food101.pth', weights_only=True))\n",
    "model.to(device)\n",
    "\n",
    "# Model loading function\n",
    "def load_model(model_type, model_path, num_classes=101):\n",
    "    if model_type == 'vit':\n",
    "        model = ViTForImageClassification.from_pretrained(model_path)\n",
    "    elif model_type == 'efficientnet':\n",
    "        model = EfficientNet.from_name('efficientnet-b2')  # Adjust according to the EfficientNet version used\n",
    "        num_features = model._fc.in_features\n",
    "        model._fc = nn.Linear(num_features, num_classes)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type. Choose 'vit' or 'efficientnet'.\")\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Model initialization\n",
    "efficientnet_model = model\n",
    "\n",
    "def evaluate_model(model, loader, model_type='vit'):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    \n",
    "    test_preds = []\n",
    "    test_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        loop = tqdm(loader, desc='[Test Evaluation]', leave=False)\n",
    "        for batch in loop:\n",
    "            inputs, labels = batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Extract logits based on model type\n",
    "            if model_type == 'vit':\n",
    "                logits = outputs.logits\n",
    "            elif model_type == 'efficientnet':\n",
    "                logits = outputs  # EfficientNetV2 returns a Tensor directly\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported model type. Choose 'vit' or 'efficientnet'.\")\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(logits, 1)\n",
    "            test_correct += torch.sum(preds == labels)\n",
    "            test_total += labels.size(0)\n",
    "\n",
    "            # Save predictions and actual labels\n",
    "            test_preds.extend(preds.cpu().numpy())\n",
    "            test_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    test_epoch_loss = test_loss / len(loader.dataset)\n",
    "    test_epoch_acc = test_correct.double() / test_total\n",
    "    test_f1 = f1_score(test_labels, test_preds, average='weighted')\n",
    "    test_precision = precision_score(test_labels, test_preds, average='weighted')\n",
    "    test_recall = recall_score(test_labels, test_preds, average='weighted')\n",
    "    return test_epoch_loss, test_epoch_acc, test_f1, test_precision, test_recall\n",
    "\n",
    "# Evaluating EfficientNetV2 model\n",
    "print(\"Evaluating EfficientNetV2 Model...\")\n",
    "eff_loss, eff_acc, eff_f1, eff_precision, eff_recall = evaluate_model(efficientnet_model, test_loader, model_type='efficientnet')\n",
    "print(f'[EfficientNetV2 Test Evaluation] - Loss: {eff_loss:.4f} | Acc: {eff_acc:.4f} | F1: {eff_f1:.4f} | Precision: {eff_precision:.4f} | Recall: {eff_recall:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d47ae444-37d5-4986-84d4-7b249ad7fff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ViTForImageClassification\n",
    "from torchvision.models import efficientnet_v2_s, EfficientNet_V2_S_Weights\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "648644e8-b513-41ef-90bd-bf481840c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes = 101  # Number of classes in the Food101 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "37f1a333-7b89-44e3-a02e-bc1da7fb5948",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViTForImageClassification(\n",
      "  (vit): ViTModel(\n",
      "    (embeddings): ViTEmbeddings(\n",
      "      (patch_embeddings): ViTPatchEmbeddings(\n",
      "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): ViTEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x ViTLayer(\n",
      "          (attention): ViTSdpaAttention(\n",
      "            (attention): ViTSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): ViTSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ViTIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ViTOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=101, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vit_model = ViTForImageClassification.from_pretrained('./vit-food101')\n",
    "vit_model.to(device)\n",
    "vit_model.eval()\n",
    "for param in vit_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0278f14f-e16e-4fc8-a0e8-e3dcb437a460",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "efficientnet_model.eval()\n",
    "for param in efficientnet_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "80e0c478-d72a-4194-801a-c3684c7d033e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): FusedMBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "      )\n",
       "      (1): FusedMBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.005, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): FusedMBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.01, mode=row)\n",
       "      )\n",
       "      (1): FusedMBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.015000000000000003, mode=row)\n",
       "      )\n",
       "      (2): FusedMBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.02, mode=row)\n",
       "      )\n",
       "      (3): FusedMBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): FusedMBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.030000000000000006, mode=row)\n",
       "      )\n",
       "      (1): FusedMBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.035, mode=row)\n",
       "      )\n",
       "      (2): FusedMBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.04, mode=row)\n",
       "      )\n",
       "      (3): FusedMBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.045, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.05500000000000001, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.06000000000000001, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.065, mode=row)\n",
       "      )\n",
       "      (4): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07, mode=row)\n",
       "      )\n",
       "      (5): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.075, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "            (1): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.085, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.09, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.095, mode=row)\n",
       "      )\n",
       "      (4): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "      )\n",
       "      (5): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.10500000000000001, mode=row)\n",
       "      )\n",
       "      (6): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.11000000000000001, mode=row)\n",
       "      )\n",
       "      (7): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.11500000000000002, mode=row)\n",
       "      )\n",
       "      (8): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.12000000000000002, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.13, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.135, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.14, mode=row)\n",
       "      )\n",
       "      (4): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.14500000000000002, mode=row)\n",
       "      )\n",
       "      (5): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.15, mode=row)\n",
       "      )\n",
       "      (6): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.155, mode=row)\n",
       "      )\n",
       "      (7): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.16, mode=row)\n",
       "      )\n",
       "      (8): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.165, mode=row)\n",
       "      )\n",
       "      (9): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.17, mode=row)\n",
       "      )\n",
       "      (10): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.175, mode=row)\n",
       "      )\n",
       "      (11): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.18, mode=row)\n",
       "      )\n",
       "      (12): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.185, mode=row)\n",
       "      )\n",
       "      (13): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.19, mode=row)\n",
       "      )\n",
       "      (14): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.195, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (7): Conv2dNormActivation(\n",
       "      (0): Conv2d(256, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=True)\n",
       "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "vit_model.to(device)\n",
    "efficientnet_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "944f0ac8-80d3-42c3-9907-c924209e905b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_366/3518493412.py:161: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  efficientnet_model.load_state_dict(torch.load('./efficientnet_food101.pth', map_location=device))\n",
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.7137, Acc: 0.8868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.2946, Acc: 0.9389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.2677, Acc: 0.9409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 0.2495, Acc: 0.9426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.2358, Acc: 0.9420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ensemble Test Evaluation] - Loss: 0.5561 | Acc: 0.8805 | F1: 0.8806 | Precision: 0.8815 | Recall: 0.8805\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Data root directory\n",
    "data_root = './data'\n",
    "\n",
    "# Number of classes\n",
    "num_classes = 101\n",
    "\n",
    "# ViT Image Processor\n",
    "feature_processor = ViTImageProcessor.from_pretrained('./vit-food101')\n",
    "\n",
    "# ViT transforms\n",
    "class ViTTransform:\n",
    "    def __init__(self, feature_processor, augment=False):\n",
    "        self.feature_processor = feature_processor\n",
    "        if augment:\n",
    "            self.augment = transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation(10),\n",
    "            ])\n",
    "        else:\n",
    "            self.augment = None\n",
    "\n",
    "    def __call__(self, image):\n",
    "        if self.augment:\n",
    "            image = self.augment(image)\n",
    "        encoding = self.feature_processor(images=image, return_tensors=\"pt\")\n",
    "        return encoding['pixel_values'].squeeze()\n",
    "\n",
    "# EfficientNet transforms\n",
    "efficientnet_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(384),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'val_test': transforms.Compose([\n",
    "        transforms.Resize(384),\n",
    "        transforms.CenterCrop(384),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Load the full training datasets\n",
    "full_train_dataset_vit = datasets.Food101(\n",
    "    root=data_root,\n",
    "    split='train',\n",
    "    transform=ViTTransform(feature_processor, augment=True),\n",
    "    download=False\n",
    ")\n",
    "\n",
    "full_train_dataset_eff = datasets.Food101(\n",
    "    root=data_root,\n",
    "    split='train',\n",
    "    transform=efficientnet_transforms['train'],\n",
    "    download=False\n",
    ")\n",
    "\n",
    "# Generate consistent train/validation indices\n",
    "num_samples = len(full_train_dataset_vit)\n",
    "indices = list(range(num_samples))\n",
    "random_seed = 42\n",
    "\n",
    "train_indices, val_indices = train_test_split(\n",
    "    indices, test_size=0.2, random_state=random_seed)\n",
    "\n",
    "# Create Subsets using the same indices for both datasets\n",
    "train_dataset_vit = Subset(full_train_dataset_vit, train_indices)\n",
    "val_dataset_vit = Subset(full_train_dataset_vit, val_indices)\n",
    "\n",
    "train_dataset_eff = Subset(full_train_dataset_eff, train_indices)\n",
    "val_dataset_eff = Subset(full_train_dataset_eff, val_indices)\n",
    "\n",
    "# Apply validation transforms to the validation datasets\n",
    "val_dataset_vit.dataset.transform = ViTTransform(feature_processor, augment=False)\n",
    "val_dataset_eff.dataset.transform = efficientnet_transforms['val_test']\n",
    "\n",
    "# Load test datasets\n",
    "test_dataset_vit = datasets.Food101(\n",
    "    root=data_root,\n",
    "    split='test',\n",
    "    transform=ViTTransform(feature_processor, augment=False),\n",
    "    download=False\n",
    ")\n",
    "\n",
    "test_dataset_eff = datasets.Food101(\n",
    "    root=data_root,\n",
    "    split='test',\n",
    "    transform=efficientnet_transforms['val_test'],\n",
    "    download=False\n",
    ")\n",
    "\n",
    "# Data loaders\n",
    "batch_size = 64\n",
    "num_workers = os.cpu_count()\n",
    "\n",
    "validation_loader_vit = DataLoader(\n",
    "    val_dataset_vit,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,  # Important: Do not shuffle to maintain order\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader_vit = DataLoader(\n",
    "    test_dataset_vit,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "validation_loader_eff = DataLoader(\n",
    "    val_dataset_eff,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,  # Important: Do not shuffle to maintain order\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader_eff = DataLoader(\n",
    "    test_dataset_eff,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Load the trained ViT model\n",
    "vit_model = ViTForImageClassification.from_pretrained('./vit-food101')\n",
    "vit_model.to(device)\n",
    "vit_model.eval()\n",
    "for param in vit_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Load the trained EfficientNet model\n",
    "efficientnet_model = models.efficientnet_v2_s(weights=None)\n",
    "num_ftrs = efficientnet_model.classifier[1].in_features\n",
    "efficientnet_model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
    "efficientnet_model.load_state_dict(torch.load('./efficientnet_food101.pth', map_location=device))\n",
    "efficientnet_model.to(device)\n",
    "efficientnet_model.eval()\n",
    "for param in efficientnet_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Function to get outputs from a model\n",
    "def get_model_outputs(model, loader, model_type='vit'):\n",
    "    outputs_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loop = tqdm(loader, desc=f'[{model_type.upper()} Evaluation]', leave=False)\n",
    "        for batch in loop:\n",
    "            inputs, labels = batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            if model_type == 'vit':\n",
    "                outputs = model(inputs)\n",
    "                logits = outputs.logits\n",
    "            elif model_type == 'efficientnet':\n",
    "                logits = model(inputs)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid model type. Choose 'vit' or 'efficientnet'.\")\n",
    "\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            outputs_list.append(probabilities.cpu())\n",
    "            labels_list.append(labels.cpu())\n",
    "\n",
    "    outputs_tensor = torch.cat(outputs_list)\n",
    "    labels_tensor = torch.cat(labels_list)\n",
    "    return outputs_tensor, labels_tensor\n",
    "\n",
    "# Get outputs for validation set\n",
    "vit_val_outputs, val_labels_vit = get_model_outputs(vit_model, validation_loader_vit, 'vit')\n",
    "eff_val_outputs, val_labels_eff = get_model_outputs(efficientnet_model, validation_loader_eff, 'efficientnet')\n",
    "\n",
    "# Ensure labels are the same\n",
    "assert torch.equal(val_labels_vit, val_labels_eff), \"Validation labels do not match!\"\n",
    "val_labels = val_labels_vit  # Use one of them since they are equal\n",
    "\n",
    "# Concatenate model outputs\n",
    "ensemble_val_features = torch.cat([vit_val_outputs, eff_val_outputs], dim=1)\n",
    "\n",
    "# Define the meta-classifier\n",
    "class MetaClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(MetaClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize meta-classifier\n",
    "input_dim = num_classes * 2  # Since we have outputs from two models\n",
    "meta_model = MetaClassifier(input_dim=input_dim, num_classes=num_classes)\n",
    "meta_model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(meta_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Create DataLoader for the meta-classifier\n",
    "val_dataset_meta = torch.utils.data.TensorDataset(ensemble_val_features, val_labels)\n",
    "val_loader_meta = torch.utils.data.DataLoader(val_dataset_meta, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Train the meta-classifier\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    meta_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    loop = tqdm(val_loader_meta, desc=f'Epoch {epoch+1}/{num_epochs} [Meta Training]', leave=False)\n",
    "    for inputs, labels in loop:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = meta_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    epoch_loss = running_loss / len(val_dataset_meta)\n",
    "    epoch_acc = correct_predictions.double() / total_predictions\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}')\n",
    "\n",
    "# Get outputs for test set\n",
    "vit_test_outputs, test_labels_vit = get_model_outputs(vit_model, test_loader_vit, 'vit')\n",
    "eff_test_outputs, test_labels_eff = get_model_outputs(efficientnet_model, test_loader_eff, 'efficientnet')\n",
    "\n",
    "# Ensure labels are the same\n",
    "assert torch.equal(test_labels_vit, test_labels_eff), \"Test labels do not match!\"\n",
    "test_labels = test_labels_vit  # Use one of them since they are equal\n",
    "\n",
    "# Concatenate model outputs for test set\n",
    "ensemble_test_features = torch.cat([vit_test_outputs, eff_test_outputs], dim=1)\n",
    "\n",
    "# Create DataLoader for the test set\n",
    "test_dataset_meta = torch.utils.data.TensorDataset(ensemble_test_features, test_labels)\n",
    "test_loader_meta = torch.utils.data.DataLoader(test_dataset_meta, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Evaluate the meta-classifier on the test set\n",
    "meta_model.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "test_preds = []\n",
    "test_labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader_meta, desc='[Meta Test Evaluation]', leave=False):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = meta_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        test_correct += torch.sum(preds == labels)\n",
    "        test_total += labels.size(0)\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        test_labels_list.extend(labels.cpu().numpy())\n",
    "\n",
    "test_epoch_loss = test_loss / len(test_dataset_meta)\n",
    "test_epoch_acc = test_correct.double() / test_total\n",
    "test_f1 = f1_score(test_labels_list, test_preds, average='weighted')\n",
    "test_precision = precision_score(test_labels_list, test_preds, average='weighted')\n",
    "test_recall = recall_score(test_labels_list, test_preds, average='weighted')\n",
    "print(f'[Ensemble Test Evaluation] - Loss: {test_epoch_loss:.4f} | Acc: {test_epoch_acc:.4f} | F1: {test_f1:.4f} | Precision: {test_precision:.4f} | Recall: {test_recall:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
